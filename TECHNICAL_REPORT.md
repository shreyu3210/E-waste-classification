# E-Waste Binary Classifier â€” Technical Report

> **Final Model Accuracy: 99.08% (Top-1) | 100% (Top-5)**  
> Trained on ~16K images | YOLOv8s-cls | 11.2 minutes

---

## 1. Problem Statement

Electronic waste (e-waste) is one of the fastest-growing waste streams globally. Improper disposal of e-waste leads to toxic chemicals leaching into soil and water. This project builds an **automated binary image classifier** that can detect whether a given item is **electronic waste** or **regular waste**, enabling smart waste sorting at recycling facilities, smart bins, or mobile apps.

### Task Definition

| Input                                   | Output                                          |
| --------------------------------------- | ----------------------------------------------- |
| Any image of a waste item (256Ã—256 RGB) | `e-waste` or `non-ewaste` with confidence score |

---

## 2. Dataset

### 2.1 Data Sources

| Source                             | Description                                                                                                                                                   | Original Images |
| ---------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------- |
| **E-Waste Dataset v44** (Roboflow) | YOLO detection dataset with 77 sub-classes of electronic waste â€” smartphones, laptops, PCBs, TVs, monitors, batteries, keyboards, mice, routers, cables, etc. | 19,525          |
| **TrashType Image Dataset**        | Folder-based classification dataset with 6 waste categories: cardboard, glass, metal, paper, plastic, trash                                                   | 2,527           |
| **Additional non-ewaste images**   | User-curated images added to the non-ewaste class to improve balance and diversity                                                                            | ~5,325          |

### 2.2 Binary Class Construction

The multi-class datasets were consolidated into a **binary classification** task:

| Class          | Composition                                                          | Description                                                                            |
| -------------- | -------------------------------------------------------------------- | -------------------------------------------------------------------------------------- |
| **e-waste**    | All images from E-Waste Dataset (labels discarded, only images used) | Phones, laptops, PCBs, TVs, batteries, monitors, keyboards, cables, etc.               |
| **non-ewaste** | All 6 garbage categories merged + additional curated images          | Cardboard, glass, metal, paper, plastic, general trash, and misc. non-electronic items |

### 2.3 Final Dataset Splits

| Split          | e-waste | non-ewaste | Total      | Ratio          |
| -------------- | ------- | ---------- | ---------- | -------------- |
| **Train**      | 8,014   | 7,852      | **15,866** | ~50.5% / 49.5% |
| **Validation** | 379     | 379        | **758**    | 50% / 50%      |
| **Test**       | 380     | 380        | **760**    | 50% / 50%      |
| **Total**      | 8,773   | 8,611      | **17,384** | â€”              |

**Class balance ratio**: 1.02:1 (nearly perfectly balanced â€” no bias issues).

### 2.4 Dataset Directory Structure

```
data/binary_dataset/
â”œâ”€â”€ train/
â”‚   â”œâ”€â”€ e-waste/         (8,014 images)
â”‚   â””â”€â”€ non-ewaste/      (7,852 images)
â”œâ”€â”€ val/
â”‚   â”œâ”€â”€ e-waste/         (379 images)
â”‚   â””â”€â”€ non-ewaste/      (379 images)
â””â”€â”€ test/
    â”œâ”€â”€ e-waste/         (380 images)
    â””â”€â”€ non-ewaste/      (380 images)
```

---

## 3. Model Architecture

### 3.1 Overview

| Property            | Value                                                          |
| ------------------- | -------------------------------------------------------------- |
| **Framework**       | Ultralytics YOLOv8 v8.4.16                                     |
| **Model Variant**   | YOLOv8s-cls (Small Classification)                             |
| **Backbone**        | Modified CSPDarknet53 with C2f (Cross Stage Partial v2) blocks |
| **Head**            | Classify head â€” Conv â†’ GAP â†’ Linear â†’ Softmax                  |
| **Total Layers**    | 56 (unfused) / 30 (fused for inference)                        |
| **Parameters**      | 5,083,298 (trainable) / 5,077,762 (fused)                      |
| **GFLOPs**          | 12.6 (training) / 12.4 (inference, fused)                      |
| **Model File Size** | 10.3 MB                                                        |
| **Pretrained On**   | ImageNet-1K (1000-class general object recognition)            |
| **Transfer Items**  | 156/158 layers transferred from pretrained weights             |

### 3.2 Layer-by-Layer Architecture

```
Layer  From   N   Params     Module                           Arguments
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  0     -1    1      928     Conv                             [3, 32, 3, 2]
  1     -1    1   18,560     Conv                             [32, 64, 3, 2]
  2     -1    1   29,056     C2f                              [64, 64, 1, True]
  3     -1    1   73,984     Conv                             [64, 128, 3, 2]
  4     -1    2  197,632     C2f                              [128, 128, 2, True]
  5     -1    1  295,424     Conv                             [128, 256, 3, 2]
  6     -1    2  788,480     C2f                              [256, 256, 2, True]
  7     -1    1 1,180,672    Conv                             [256, 512, 3, 2]
  8     -1    1 1,838,080    C2f                              [512, 512, 1, True]
  9     -1    1  660,482     Classify                         [512, 2]
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                  5,083,298  Total parameters
```

### 3.3 Data Flow Diagram

```
Input Image (256Ã—256Ã—3)
    â”‚
    â”œâ”€â”€ Layer 0: Conv2d(3â†’32, k=3, s=2) + BN + SiLU
    â”‚   Output: 128Ã—128Ã—32
    â”‚
    â”œâ”€â”€ Layer 1: Conv2d(32â†’64, k=3, s=2) + BN + SiLU
    â”‚   Output: 64Ã—64Ã—64
    â”‚
    â”œâ”€â”€ Layer 2: C2f(64â†’64, n=1, shortcut=True)
    â”‚   â”œâ”€â”€ Split â†’ Bottleneck (Conv 3Ã—3 â†’ Conv 3Ã—3) â†’ Concat
    â”‚   Output: 64Ã—64Ã—64
    â”‚
    â”œâ”€â”€ Layer 3: Conv2d(64â†’128, k=3, s=2) + BN + SiLU
    â”‚   Output: 32Ã—32Ã—128
    â”‚
    â”œâ”€â”€ Layer 4: C2f(128â†’128, n=2, shortcut=True)
    â”‚   â”œâ”€â”€ Split â†’ 2Ã— Bottleneck â†’ Concat
    â”‚   Output: 32Ã—32Ã—128
    â”‚
    â”œâ”€â”€ Layer 5: Conv2d(128â†’256, k=3, s=2) + BN + SiLU
    â”‚   Output: 16Ã—16Ã—256
    â”‚
    â”œâ”€â”€ Layer 6: C2f(256â†’256, n=2, shortcut=True)
    â”‚   â”œâ”€â”€ Split â†’ 2Ã— Bottleneck â†’ Concat
    â”‚   Output: 16Ã—16Ã—256
    â”‚
    â”œâ”€â”€ Layer 7: Conv2d(256â†’512, k=3, s=2) + BN + SiLU
    â”‚   Output: 8Ã—8Ã—512
    â”‚
    â”œâ”€â”€ Layer 8: C2f(512â†’512, n=1, shortcut=True)
    â”‚   â”œâ”€â”€ Split â†’ Bottleneck â†’ Concat
    â”‚   Output: 8Ã—8Ã—512
    â”‚
    â””â”€â”€ Layer 9: Classify Head
        â”œâ”€â”€ Conv2d(512â†’1024, k=1) + BN + SiLU  â†’  8Ã—8Ã—1024
        â”œâ”€â”€ Global Average Pooling              â†’  1Ã—1Ã—1024
        â”œâ”€â”€ Dropout(p=0.0)                      â†’  1024
        â”œâ”€â”€ Linear(1024 â†’ 2)                    â†’  2
        â””â”€â”€ Softmax                             â†’  [P(e-waste), P(non-ewaste)]
```

### 3.4 Key Architectural Components

| Component                  | What It Does                                                                                                                                | Why It Matters                                                         |
| -------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------- |
| **C2f Block**              | Cross Stage Partial v2 â€” splits features into two paths, one goes through bottleneck layers, then fuses. More efficient than ResNet blocks. | Extracts rich multi-scale features with fewer parameters               |
| **SiLU (Swish)**           | Activation function: `x Ã— Ïƒ(x)`. Smooth, non-monotonic.                                                                                     | Better gradient flow than ReLU, especially in deep networks            |
| **Batch Normalization**    | Normalizes activations per mini-batch after each conv layer                                                                                 | Stabilizes training, allows higher learning rates                      |
| **Global Average Pooling** | Averages each channel's spatial map to a single value                                                                                       | Reduces 8Ã—8Ã—1024 â†’ 1024 without FC explosion, also acts as regularizer |
| **Layer Fusion**           | Merges Conv + BN into a single layer at inference                                                                                           | Reduces layers from 56 â†’ 30, speeds up inference                       |

---

## 4. Training Configuration

### 4.1 Optimizer

| Property             | Value                 | Rationale                                                      |
| -------------------- | --------------------- | -------------------------------------------------------------- |
| **Optimizer**        | AdamW                 | Decoupled weight decay; fast convergence for transfer learning |
| **Initial LR (lr0)** | 1 Ã— 10â»Â³              | Standard for fine-tuning pretrained models                     |
| **Final LR (lrf)**   | 0.01 Ã— lr0 = 1 Ã— 10â»âµ | Gentle final learning rate for fine details                    |
| **Momentum (Î²â‚)**    | 0.937                 | High momentum for stable gradients                             |
| **Î²â‚‚**               | 0.999                 | Default Adam second moment                                     |
| **Weight Decay**     | 0.01                  | L2 regularization, decoupled in AdamW                          |
| **Warmup Epochs**    | 3.0                   | Gradual LR ramp-up prevents early divergence                   |
| **Warmup Momentum**  | 0.8                   | Lower initial momentum during warmup                           |
| **Warmup Bias LR**   | 0.1                   | Separate bias warmup for stability                             |

**Parameter Groups:**
| Group | Count | Weight Decay |
|-------|-------|-------------|
| Weights (non-decay) | 26 | 0.0 |
| Weights (decay) | 27 | 0.01 |
| Biases | 27 | 0.0 |

### 4.2 Learning Rate Schedule

**Cosine annealing** from `lr0` to `lr0 Ã— lrf`:

```
LR(t) = lrf + 0.5 Ã— (1 - lrf) Ã— (1 + cos(Ï€ Ã— t / T))
```

Where `t` = current epoch, `T` = total epochs. The schedule includes a linear warmup for the first 3 epochs.

### 4.3 Loss Function

| Loss                 | Formula               | Description                                                                                                                          |
| -------------------- | --------------------- | ------------------------------------------------------------------------------------------------------------------------------------ |
| **CrossEntropyLoss** | `L = -Î£ yáµ¢ Â· log(Å·áµ¢)` | Standard loss for multi-class classification with softmax output. For binary case, equivalent to: `L = -[yÂ·log(p) + (1-y)Â·log(1-p)]` |

### 4.4 Training Hyperparameters

| Parameter         | Value     | Rationale                                                   |
| ----------------- | --------- | ----------------------------------------------------------- |
| **Epochs**        | 10        | Sufficient for transfer learning with balanced data         |
| **Batch Size**    | 32        | Fits comfortably in GPU VRAM with AMP enabled               |
| **Image Size**    | 256 Ã— 256 | Good balance of detail vs. speed for classification         |
| **Workers**       | 8         | Keeps GPU fed without system lag                            |
| **Patience**      | 5         | Early stopping if val accuracy doesn't improve for 5 epochs |
| **AMP (FP16)**    | Enabled   | Mixed precision â€” halves memory, speeds up training         |
| **Deterministic** | True      | Reproducible results                                        |
| **Seed**          | 0         | Default random seed                                         |

---

## 5. Data Preprocessing & Augmentation

### 5.1 Training Augmentations (Applied Automatically)

| Augmentation        | Parameter | Effect                                                    |
| ------------------- | --------- | --------------------------------------------------------- |
| **Resize**          | 256 Ã— 256 | All images resized to uniform square input                |
| **HSV Hue**         | h = 0.015 | Random hue shift Â±1.5% â€” color variation robustness       |
| **HSV Saturation**  | s = 0.7   | Random saturation Â±70% â€” lighting robustness              |
| **HSV Value**       | v = 0.4   | Random brightness Â±40% â€” exposure robustness              |
| **Horizontal Flip** | p = 0.5   | 50% chance â€” position invariance                          |
| **Vertical Flip**   | p = 0.0   | Disabled (waste items are usually upright)                |
| **Scale**           | 0.5       | Random scale Â±50% â€” size invariance                       |
| **Translate**       | 0.1       | Random shift Â±10% â€” position invariance                   |
| **Mosaic**          | p = 1.0   | Combines 4 images into one â€” regularization + multi-scale |
| **Random Erasing**  | p = 0.4   | Randomly erases patches â€” occlusion robustness            |
| **RandAugment**     | auto      | Randomly applies augmentation policies from a pool        |

### 5.2 Inference Preprocessing

| Step                 | Details                                                   |
| -------------------- | --------------------------------------------------------- |
| 1. **Read**          | Load image from file / webcam frame / upload              |
| 2. **Resize**        | Bilinear interpolation to 256 Ã— 256                       |
| 3. **Normalize**     | Pixel values Ã· 255 â†’ [0.0, 1.0]                           |
| 4. **Channel Order** | BGR â†’ RGB                                                 |
| 5. **Tensor Format** | HWC â†’ CHW (HeightÃ—WidthÃ—Channels â†’ ChannelsÃ—HeightÃ—Width) |
| 6. **Data Type**     | float16 (FP16 with AMP) or float32                        |
| 7. **Batch**         | Add batch dimension â†’ [1, 3, 256, 256]                    |

---

## 6. Transfer Learning Strategy

```
ImageNet-1K Pretrained Model (1000 classes)
    â”‚
    â”‚  156/158 layers transferred
    â”‚  (all backbone + most of head)
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Replace Classification Head    â”‚
â”‚  Linear(1024 â†’ 1000)           â”‚  â† removed
â”‚  Linear(1024 â†’ 2)              â”‚  â† new (e-waste, non-ewaste)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚
    â”‚  Full fine-tuning (all layers trainable)
    â”‚  Lower LR (1e-3) preserves pretrained features
    â”‚
    â–¼
  Fine-tuned E-Waste Classifier
```

**Why transfer learning?**

- ImageNet pretraining provides powerful low-level features (edges, textures, shapes) that transfer well to waste classification
- Only ~16K images â€” training from scratch would require 100K+ images for similar accuracy
- Reduces training time from hours to **11 minutes**

---

## 7. GPU Optimization Techniques

| Technique                 | Impact                        | Details                                                                   |
| ------------------------- | ----------------------------- | ------------------------------------------------------------------------- |
| **Mixed Precision (AMP)** | ~50% memory savings           | Forward pass in FP16, backward in FP32. Mantissa reduced from 23â†’10 bits. |
| **Small Model (YOLOv8s)** | 92% fewer params than XL      | 5.1M vs 57M parameters. 12.4 vs 154 GFLOPs.                               |
| **Image Size 256**        | 6.25Ã— less pixels than 640    | Classification doesn't need detection-level resolution.                   |
| **Batch Size 32**         | Optimal throughput            | Larger batches â†’ more stable gradients, better GPU utilization.           |
| **8 Workers**             | CPU pipeline saturated        | Multiple cores loading data â€” GPU never starved.                          |
| **Layer Fusion**          | 46% fewer layers at inference | Conv + BN merged â€” fewer kernel launches.                                 |

---

## 8. Training Results

### 8.1 Training Curves

![Training Curves](training_curves.png)

_Figure: Training Loss (left), Top-1 Validation Accuracy (center), and Learning Rate Schedule (right) across 10 epochs._

### 8.2 Epoch-wise Metrics

| Epoch  | Loss       | Top-1 Accuracy | Top-5 Accuracy |
| ------ | ---------- | -------------- | -------------- |
| 1      | 0.1471     | 97.8%          | 100%           |
| 2      | 0.1006     | 97.0%          | 100%           |
| 3      | 0.0873     | 98.3%          | 100%           |
| 4      | 0.0729     | 97.9%          | 100%           |
| 5      | 0.0547     | 98.0%          | 100%           |
| 6      | 0.0417     | 98.2%          | 100%           |
| 7      | 0.0320     | 97.9%          | 100%           |
| 8      | 0.0282     | 98.5%          | 100%           |
| 9      | 0.0227     | 98.8%          | 100%           |
| **10** | **0.0176** | **99.1%**      | **100%**       |

### 8.3 Confusion Matrix

![Confusion Matrix](confusion_matrix.png)

_Figure: Confusion matrix with raw counts (left) and normalized values (right). The model achieves 0 false positives and only 7 false negatives._

### 8.4 Detailed Classification Metrics

| Metric                   | Value      | Formula                                              |
| ------------------------ | ---------- | ---------------------------------------------------- |
| **True Positives (TP)**  | 372        | E-waste correctly identified as e-waste              |
| **True Negatives (TN)**  | 379        | Non-ewaste correctly identified as non-ewaste        |
| **False Positives (FP)** | 0          | Non-ewaste incorrectly classified as e-waste         |
| **False Negatives (FN)** | 7          | E-waste incorrectly classified as non-ewaste         |
| **Accuracy**             | **99.08%** | (TP + TN) / Total = 751 / 758                        |
| **Precision**            | **100.0%** | TP / (TP + FP) = 372 / 372                           |
| **Recall (Sensitivity)** | **98.15%** | TP / (TP + FN) = 372 / 379                           |
| **F1-Score**             | **99.07%** | 2 Ã— Precision Ã— Recall / (Precision + Recall)        |
| **Specificity**          | **100.0%** | TN / (TN + FP) = 379 / 379                           |
| **Top-5 Accuracy**       | **100.0%** | Always correct within top-5 (binary, trivially 100%) |
| **Training Time**        | 11.2 min   | 0.187 hours across 10 epochs                         |
| **Best Model Size**      | 10.3 MB    | Optimizer stripped from checkpoint                   |

**Key Observations:**

- **Zero false positives** â€” the model never misclassifies non-ewaste as e-waste
- Only **7 out of 379 e-waste images** (1.85%) were missed (classified as non-ewaste)
- Perfect **specificity** (100%) ensures no false alarms in production

### 8.5 Inference Speed

| Stage            | Time per Image         |
| ---------------- | ---------------------- |
| Preprocessing    | 0.2 ms                 |
| **Inference**    | **2.1 ms**             |
| Loss computation | 0.0 ms                 |
| Postprocessing   | 0.0 ms                 |
| **Total**        | **~2.3 ms (~435 FPS)** |

---

## 9. Deployment

### 9.1 Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    CLIENT (Browser)                  â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  ðŸ“ Upload    â”‚          â”‚  ðŸ“¹ Webcam           â”‚  â”‚
â”‚  â”‚  (drag/drop)  â”‚          â”‚  (1.5s intervals)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â”‚ POST /predict               â”‚ POST /predict-frame
â”‚         â”‚ (multipart file)            â”‚ (base64 JSON)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚            HTTP             â”‚
          â–¼                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Server (port 8000)              â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚            YOLOv8s-cls Model                   â”‚  â”‚
â”‚  â”‚         (loaded once at startup)               â”‚  â”‚
â”‚  â”‚                                                â”‚  â”‚
â”‚  â”‚  Image/Frame â†’ PIL.Image â†’ YOLO predict()     â”‚  â”‚
â”‚  â”‚  â†’ Softmax â†’ Top-5 probs â†’ JSON response     â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                     â”‚
â”‚  Response JSON:                                     â”‚
â”‚  {                                                  â”‚
â”‚    "prediction": "e-waste" | "non-ewaste",         â”‚
â”‚    "confidence": 97.5,                              â”‚
â”‚    "is_ewaste": true | false,                       â”‚
â”‚    "top5": [{class, confidence, emoji, color}, ...] â”‚
â”‚  }                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 API Endpoints

| Method | Endpoint         | Input                   | Output                     |
| ------ | ---------------- | ----------------------- | -------------------------- |
| `GET`  | `/`              | â€”                       | HTML frontend              |
| `POST` | `/predict`       | Multipart image file    | JSON classification result |
| `POST` | `/predict-frame` | Base64 JPEG (JSON body) | JSON classification result |

### 9.3 Frontend Features

- **Upload mode**: Drag-and-drop or click to upload. Shows image preview and animated confidence bars.
- **Webcam mode**: Real-time classification every 1.5 seconds with live overlay and detailed results.
- **E-waste alert**: Pulsing red banner when e-waste is detected with >50% confidence.
- **Dark theme**: Modern glassmorphism design with gradient animations.

---

## 10. File Structure

```
Projecr/
â”œâ”€â”€ test.py                  # Training pipeline
â”œâ”€â”€ predict.py               # CLI inference (image / folder / webcam / test)
â”œâ”€â”€ app.py                   # FastAPI server
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html           # Web UI (upload + webcam)
â”œâ”€â”€ TECHNICAL_REPORT.md      # This document
â”œâ”€â”€ yolov8s-cls.pt           # Base pretrained model (auto-downloaded)
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ E-waste/             # Original YOLO detection dataset
â”‚   â”œâ”€â”€ garbage/             # Original TrashType dataset
â”‚   â””â”€â”€ binary_dataset/      # Final binary classification dataset
â”‚       â”œâ”€â”€ train/           #   e-waste (8,014) + non-ewaste (7,852)
â”‚       â”œâ”€â”€ val/             #   e-waste (379) + non-ewaste (379)
â”‚       â””â”€â”€ test/            #   e-waste (380) + non-ewaste (380)
â”‚
â””â”€â”€ runs/classify/ewaste_binary/
    â”œâ”€â”€ weights/
    â”‚   â”œâ”€â”€ best.pt          # Best checkpoint (99.08% accuracy)
    â”‚   â””â”€â”€ last.pt          # Final epoch checkpoint
    â”œâ”€â”€ results.csv          # Per-epoch training metrics
    â”œâ”€â”€ results.png          # Training curves plot
    â”œâ”€â”€ confusion_matrix.png # Confusion matrix visualization
    â””â”€â”€ args.yaml            # Complete training configuration
```

---

## 11. Environment & Reproducibility

### 11.1 Software Stack

| Component        | Version    |
| ---------------- | ---------- |
| **Python**       | 3.10+      |
| **PyTorch**      | 2.x + CUDA |
| **Ultralytics**  | 8.4+       |
| **FastAPI**      | 0.100+     |
| **Uvicorn**      | latest     |
| **Pillow (PIL)** | (bundled)  |

### 11.2 Reproduction Steps

```bash
# 1. Activate virtual environment
env\scripts\activate

# 2. Install dependencies
pip install ultralytics fastapi uvicorn python-multipart

# 3. Train the model
python test.py

# 4. Test via CLI
python predict.py --test-samples 10
python predict.py path\to\image.jpg
python predict.py --webcam

# 5. Start web server
python app.py
# Open http://localhost:8000
```

---

## 12. Potential Improvements

| Improvement                      | Expected Impact                                                         |
| -------------------------------- | ----------------------------------------------------------------------- |
| **More non-ewaste data**         | Better generalization to diverse waste types                            |
| **YOLOv8m-cls** (medium model)   | Higher accuracy, more parameters for better feature extraction          |
| **More epochs (20-30)**          | Loss was still decreasing at epoch 10 â€” more training could help        |
| **Test-Time Augmentation (TTA)** | Predict on multiple augmented versions and average â€” +1-2% accuracy     |
| **Grad-CAM visualization**       | Visualize which image regions the model focuses on for interpretability |
| **ONNX/TensorRT export**         | 2-5Ã— faster inference for production deployment                         |
| **Mobile deployment**            | Export to TFLite for Android/iOS smart bin applications                 |
